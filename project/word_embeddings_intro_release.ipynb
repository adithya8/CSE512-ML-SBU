{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word embeddings**\n",
    "\n",
    "In this exercise we will experiment with natural language processing using word embeddings. Later, we will attempt at part-of-speech classification, using the word vectors themselves as feature vectors.\n",
    "\n",
    "After completing this notebook, please copy the following into your PDF report:\n",
    "\n",
    "* a frequency histogram, and a histogram of the co-occurances of the words in the dataset\n",
    "* the list of the 10 most frequent words, and the 10 least frequent words\n",
    "* the list of the 10 most cooccuring word pairs\n",
    "* the 10 closest words to the city where you were born. (If you were born in a town which is not in the vocabulary, pick the closest city which *is* in the vocabulary.\n",
    "* the 10 closest words to an object that is close to you right now.\n",
    "* your 2-D PCA word embeddings, and some comments on any interesting geometric structure you may see\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Yifan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import packages (may take some time)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "import scipy.sparse.linalg as ssl\n",
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "import scipy.sparse as ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**\n",
    "\n",
    "The following files are some counts matrices/vectors that were accrued by scraping the wikipedia dataset, archived from 2000 (enwiki 2000). They have already been preprocessed somewhat; namely\n",
    "\n",
    "* all \"stopwords\", or words that are far too common (see stopwords.txt) have been removed\n",
    "* rare words (occuring fewer than 10000 times in the corpus) have been removed\n",
    "\n",
    "Use at least 100 bins in your histogram, to really see the distribution. Remember to set the minimum bin range to be 10000, or you will have a huge spike at 0.\n",
    "\n",
    "Take a look at these files, and plot a histogram of the word frequencies, and the cooccurance numbers. Note that these figures are actually *less* sharp than it would be over a non-preprocessed corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = h5py.File('cooccurance_stop0_minfreq10000_skipwindow5.mat')\n",
    "cooccurance = np.array(f['wordcontext'])\n",
    "wordfreq = sio.loadmat('wordfreq_update.mat')\n",
    "freq = wordfreq['freq'][0,:].astype(float)\n",
    "wordlist = wordfreq['word'][0,:]\n",
    "wordlist = [word[0] for word in wordlist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stats**\n",
    "\n",
    "Play around with these statistics. Print out\n",
    "* the 10 most frequent and least frequent words\n",
    "* the 10 most frequent pairs of words. (Be sure to get rid of repeated words and pairs that occur twice (a,b) = (b,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PPMI matrix**\n",
    "\n",
    "Now construct a PPMI matrix defined as follows\n",
    "\n",
    "$$PMI_{ij} = \\log\\left(\\frac{corpus size \\cdot coocurrance_{ij}}{ (freq(i)\\cdot freq(j) + 1.)}\\right), \\qquad PPMI = \\max\\{PMI,0\\}$$\n",
    "\n",
    "(here, the + 1 is used to prevent dividing by 0)\n",
    "\n",
    "Form word embeddings of dimension 300 by taking the eigenvalue decomposition of PPMI and removing the eigenvalue/eigenvector pairs which are not amongst the largest 300 eigenvalues. Then form the factorization as \n",
    "\n",
    "$$PPMI = U D U^T, \\qquad V = UD^{1/2}$$\n",
    "\n",
    "Hold onto these vectors $V$; they will later be your feature vectors (for each word, a feature vector of length 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nearest K words**\n",
    "\n",
    "For fun, let's take a look at the geometry of these words! Create a function that returns the \"k closest words\" via cosine similarity:\n",
    "\n",
    "$$cossim(word_i, word_j) = \\frac{v_i^Tv_j}{\\|v_i\\|_2\\|v_j\\|}$$\n",
    "\n",
    "That is, given a word, find the 10 words with the highest cossim with the query word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'aaron', u'ab', u'abandon', u'abandoned', u'abbey', u'abbot', u'abbreviated', u'abc', u'abdul', u'aberdeen']\n",
      "[u'aaron', u'ab', u'abandon', u'abandoned', u'abbey', u'abbot', u'abbreviated', u'abc', u'abdul', u'aberdeen']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_k_closest_words(word, k):\n",
    "    return range(k)\n",
    "\n",
    "print [wordlist[i] for i in get_k_closest_words('shanghai',10)]\n",
    "print [wordlist[i] for i in get_k_closest_words('coffee',10)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization**\n",
    "\n",
    "To make some pretty plots, do a PCA on your word embedding. First normalize by setting the mean word embedding to 0. Then extract the largest 2 eigenvalue/eigenvector pairs, and plot them. \n",
    "\n",
    "The plot itself gets a bit overwhelming if you try to plot all the words at once. Instead, pick some fun word, and plot the 100 words closest too it, using your previous code. (Use plt.text to plot the texts in 2-D space.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vec_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8208d1efbfbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mplot_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec_pca\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvec_pca\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'vec_pca' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAA5FJREFUeJzt1MENwCAQwLDS/Xc+tgCJ2BPklTUzHwDv+28HAHCG4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QMQGL4sE9RSocXsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plot_words = get_k_closest_words('coffee', 100)\n",
    "\n",
    "for k in plot_words:\n",
    "    plt.text(vec_pca[k,0],vec_pca[k,1], wordlist[k], fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part of speech label**\n",
    "\n",
    "Finally, we use an online POS tagger to label our words. We will then save everything into a dataset, which we can then use for some fun machine learning tasks!\n",
    "\n",
    "Run the code given in the box.  Using the example, construct a train set (word embeddings) and test set (number representing part of speech), and save it somehow (using sio.savemat or pickle.dump). \n",
    "\n",
    "To verify that everything went correctly, the following is the counts per part-of-speech:\n",
    "\n",
    "* NOUN 3654\n",
    "* VERB 1678\n",
    "* ADJ 1497\n",
    "* ADV 254\n",
    "* ADP 24\n",
    "* NUM 4\n",
    "* DET 1\n",
    "* PRT 3\n",
    "* X 23\n",
    "* PRON 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', u'PRON'), ('really', u'ADV'), ('need', u'VERB'), ('my', u'PRON'), ('coffee', u'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print nltk.pos_tag(['I','really','need','my','coffee'],'universal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
