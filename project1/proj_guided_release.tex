\documentclass{article}
\usepackage{tabularx,fullpage,url}
\usepackage[top=1in, bottom=1in, left=.5in, right=.75in]{geometry}
\usepackage{amsmath,amssymb,graphicx,amsthm,xparse, color, mathrsfs} 
\usepackage{ epstopdf, fullpage}

\usepackage{xifthen}
\usepackage[ruled,vlined]{algorithm2e}

\newcommand{\mypagebreak}{\begin{center}
		\noindent\makebox[\linewidth]{\rule{7.5in}{1pt}}
	\end{center}}
\bibliographystyle{siam}
\newcommand{\minimize}[1]{\underset{#1}{\text{minimize}}}
\newcommand{\maximize}[1]{\underset{#1}{\text{maximize}}}

\newcommand{\mini}[1]{\underset{#1}{\text{min}}}
\newcommand{\argmin}[1]{\underset{#1}{\text{argmin}}}
\newcommand{\st}{\text{subject to}}
\newcommand{\rank}{\textbf{rank}}
\newcommand{\epi}{\mathbf{epi}}

\newcommand{\diag}{\textbf{diag}}
\newcommand{\mb}{\mathbf}
\newcommand{\R}{\mathbb R}
\newcommand{\mle}{\mathbf{MLE}}
\newcommand{\map}{\mathbf{MAP}}
\newcommand{\bE}{\mathbb E}
\newcommand{\mL}{\mathcal L}
\newcommand{\mH}{\mathcal H}
\newcommand{\mN}{\mathcal N}
\newcommand{\mD}{\mathcal D}
\newcommand{\mC}{\mathcal C}

\newcommand{\mS}{\mathcal S}
\newcommand{\tr}{\mathbf{tr}}
\newcommand{\mrm}{\mathrm}
\newcommand{\proj}{\mathbf{proj}}
\newcommand{\conv}{\mathbf{conv}}
\newcommand{\prox}{\mathbf{prox}}
\newcommand{\sign}{\mathbf{sign}}
\newcommand{\range}{\mathbf{range}}
\newcommand{\var}{\mathbf{var}}
\newcommand{\vnull}{\mathbf{null}}
\newcommand{\pr}{\mathbf{Pr}}
\newcommand{\find}{\mathbf{find}}
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}}}
\newcommand{\subjto}{\mathrm{subject~to}}

\newcommand{\bmat}{\left[\begin{matrix}}
\newcommand{\emat}{\end{matrix}\right]}

\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}

\newcommand{\gray}[1]{\textcolor{lightgray}{#1}}


\newcommand{\ifequals}[3]{\ifthenelse{\equal{#1}{#2}}{#3}{}}
\newcommand{\case}[2]{#1 #2} 
\newenvironment{switch}[1]{\renewcommand{\case}{\ifequals{#1}}}{}





\begin{document}
{\Large\textbf{CSE 512: Guided project:  \hfill
Due Dec. 7 }}


\mypagebreak



\section{Word embeddings}

Open the ipython notebook \texttt{word\_embedding\_intro\_release.ipynb}. Include in your writeup

\begin{itemize}
\item  a frequency histogram, and a histogram of the co-occurances of the words in the dataset
\item  the list of the 10 most frequent words, and the 10 least frequent words
\item  the list of the 10 most cooccuring word pairs
\item  the 10 closest words to the city where you were born. (If you were born in a town which is not in the vocabulary, pick the closest city which *is* in the vocabulary.
\item  the 10 closest words to an object that is close to you right now.
\item  your 2-D PCA word embeddings, and some comments on any interesting geometric structure you may see
\end{itemize} 


\section{Duality theory and SVMs}

\begin{itemize}

\item 
Remember the soft-margin SVM problem from forever ago?
\begin{equation}
\begin{array}{ll}
\minimize{\theta,s}  & \displaystyle \tfrac{1}{2} \|\theta\|_2^2 + C \sum_{i=1}^m \max\{s_i,0\}\\
\subjto & y_ix_i^T\theta  + s_i = 1,\;  i = 1,...,m
\end{array}
\label{eq:svmprimal1}
\end{equation}
We're going to use this method to classify our parts of speech.


\item \textbf{Subgradients and subdifferentials.} The hinge loss function 
\[
g(s) = \sum_{i=1}^m \max\{s_i,0\}
\]
is unfortunately not differentiable whenever there exists one $s_i = 0$. But, it is convex. So, we can still take descent steps with respect to the objective by looking for \emph{subgradients}. 
Specifically, for the scalar function 
\[
g_i(s) = \max\{s_i,0\},
\]
we describe the \emph{subdifferential} of $g_i$ at $s$ as 
\[
\partial g_i(s) = \conv\left\{\lim_{\epsilon\to 0^+} g'_i(s+\epsilon),\lim_{\epsilon\to 0^-} g'_i(s+\epsilon)\right\} = 
\begin{cases}
\{1\} & s_i > 0\\
\{0\} & s_i < 0\\
[0,1] & s_i = 0.
\end{cases}
\]
Since subdifferentials are linear (not obvious, but can be proved), we can summarize the subdifferential of the hinge function via 
\[
\partial (C\cdot g(s)) = C\sum_{i=1}^m \partial g_i(s)
\]
where we describe the transformation of sets as 
\[
C\mS = \{Cx : x\in \mS\} \text{ and } \mS_1 + \mS_2 = \{x+y : x\in \mS_1, \; y \in \mS_2\}.
\]

\item
The subgradient method for minimizing $f(x)$ for some convex but not differentiable function $f$ goes as 
\[
x^{(t+1)} = x^{(t)} - \alpha^{(t)} g^{(t)}, \quad g^{(t)}\in \partial f(x^{(t)}).
\]
Here, $g^{(t)}$ is any element from $\partial f(x^{(t)})$. When you have more than one choice, you can pick any element, but you should design a rule that is agnostic to your knowledge of where the true solution is. When $\alpha^{(t)}$ is diminishing but not summable, e.g. 
\[
\alpha^{(t+1)} < \alpha^{(t)}, \qquad \lim_{t\to +\infty} \sum_{i=1}^t \alpha^{(i)} = \infty
\]
then the subgradient method is known to converge. In particular, picking $\alpha^{(t)} = 1/(Lt)$, the method converges at a rate of  $f(x^{(t)}) - f^* = O(1/\sqrt{(t)})$.


\item[\textbf{Q1}] Show that for a \emph{constant} step size, subgradient descent doesn't converge. Show this by arguing that minimizing the function 
\[
\minimize{x} \quad \frac{1}{2}(x-c)^2 + \max\{x,0\} 
\]
cannot reach its optimum for \emph{all} choices of $c$, using a step size that is agnostic to $c$. 

Specifically, do this by picking a step size reference point $0 < \bar s < 2/L$, a starting point $x^{(0)} \neq x^*$, and some value of $c$, and show via simulation that $x^{(k)}$ doesn't converge for a step size of $\bar s$, $\bar s/10$, and $\bar s/100$. 





\item \textbf{Projections.} Consider the projection problem 
\begin{equation}
\begin{array}{ll}
\minimize{x} & \frac{1}{2}\|x-\hat x\|_2^2\\
\subjto & Ax = b
\end{array}
\label{eq:projection-primal}
\end{equation}
\item[\textbf{Q2}] Find the Lagrange dual of \eqref{eq:projection-primal} and use the solution of the dual to show that, assuming that $AA^T$ is invertible, then the solution to \eqref{eq:projection-primal} is 
\[
x = A^T(AA^T)^{-1} (b-A\hat x) + \hat x.
\]

\item \textbf{Projected subgradient descent.} For a problem of form 
\[
\begin{array}{ll}
\minimize{x} & f(x) \\
\subjto & Ax = b
\end{array}
\]
the projected subgradient descent method runs the iterative scheme (from any initial point)
\[
x^{(t+1)} = \proj_{\mH}(x^{(t)} - \alpha g^{(t)})
\]
where $g^{(t)}$ is a subgradient of $f$ at $x^{(t)}$ and 
$\mH = \{x : Ax = b\}$ and the projection of $\hat x$ on $\mH$ is the solution to problem \eqref{eq:projection-primal}.


\item[\textbf{Q3}] Use the pieces you have so far to propose an iteration scheme to solve \eqref{eq:svmprimal1} via projected subgradient descent. That is, given some $\theta^{(t)}$ and $s^{(t)}$, write out the equations used to compute $\theta^{(t+1)}$ and $s^{(t+1)}$, as explicitly as possible. (You will implement this code in the next section, so do it carefully here.)

\item An equivalent formulation of \eqref{eq:svmprimal1} is the following 
\begin{equation}
\begin{array}{ll}
\minimize{\theta,s}  & \displaystyle \tfrac{1}{2} \|\theta\|_2^2 + C \sum_{i=1}^m s_i\\
\subjto & y_ix_i^T\theta  + s_i \geq 1,\;  i = 1,...,m\\
& s_i \geq 0.
\end{array}
\label{eq:svmprimal2}
\end{equation}
\item[\textbf{Q4}] Write down the Lagrangian saddle point problem of \eqref{eq:svmprimal2} and minimize it with respect to the primal variables to derive the following dual problem
\begin{equation}
\begin{array}{ll}
\maximize{u}  & \displaystyle -\tfrac{1}{2} \sum_{i=1}^m\sum_{j=1}^m y_iy_jx_i^Tx_ju_iu_j  +  \sum_{i=1}^m u_i\\
\subjto & 0 \leq u \leq C.
\end{array}
\label{eq:svmdual}
\end{equation}
\item[\textbf{Q5}] Show that if  $\theta^*$, $s^*$ optimize \eqref{eq:svmprimal2} and $u^*$ optimizes \eqref{eq:svmdual}, then
\begin{enumerate}
\item $s_i^* > 1$ implies the $i$th training sample is not classified correctly,
\item $y_ix_i^T\theta > 1$ implies $u_i = 0$,
\item $y_ix_i^T\theta < 1$ implies $u_i = C$,
\item $0 < u_i < C$ implies $y_ix_i^T\theta = 1$.
\end{enumerate}
\item[\textbf{Q6}] Given $u^*$, propose a predictor, e.g. some function where $f(u^*, x) = x^T\theta^* $.


\item[\textbf{Q7}] Propose how you would solve this using projected gradient descent, and how you would form a predictor. Remember that later, you will code this up, so include any detail here you would need to write this code.

For implementing the projection on the constraints, use the following convex optimization fact:

\begin{center}
\fbox{
\begin{minipage}{.7\textwidth}
The projection on a \emph{box constraint} 
\[
\mS = \{x\in \R^n : a_i \leq x_i \leq b_i, \; i = 1,...,n\}
\]
can be implemented elementwise as 
\[
\proj_{\mS}(\hat x)_i = \begin{cases}
a_i, & \text{ if } \hat x_i < a_i\\
b_i, & \text{ if } \hat x_i > b_i\\
\hat x_i, & \text{ else.}
\end{cases}
\]
\end{minipage}
}
\end{center}

\item\textbf{Kernel support vector machines} One way to think about kernel functions is to replace the linear prediction 
\[
y = \sign(x_i^T\theta)
\]
with a lifted version of $x_i$, as $\phi(x_i)$, e.g. 
\[
y = \sign(\phi(x_i)^T\theta).
\]
After finding the dual, this gives us an objective function of 
\[
g(u) = -\tfrac{1}{2} \sum_{i=1}^m (y_i\phi(x_i)^Tu)^2  +  \sum_{i=1}^m u_i = -\tfrac{1}{2} u^TKu + u^T\mb 1
\]
where $K_{ij} = y_iy_j\phi(x_i)^T\phi(x_j)$ defines the Kernel matrix. The Kernel trick is simply, instead of holding onto the representations $\phi(x)$, to only hold onto these inner product values. For example, the radial basis function (RBF) kernel computes 
\[
K_{ij} = K(x_i,x_j) = \exp\left(-\frac{\|x_i-x_j\|_2}{2\sigma^2}\right), \qquad \hat K_{ij} = K_{ij}y_iy_j
\]
which can be precomputed. 
\begin{equation}
\begin{array}{ll}
\maximize{u}  & \displaystyle -\tfrac{1}{2} \sum_{i=1}^m u^T\hat Ku^2  +  \sum_{i=1}^m u_i\\
\subjto & 0 \leq u \leq C.
\end{array}
\label{eq:svmdual-kernel}
\end{equation}


\item[\textbf{Q8}] Propose how, given $u^*$, you would offer a prediction on a new datasample, $x$. Remember that you do not have access to $\phi$, but you do have access to the kernel function $\mathcal K$. 

\item[\textbf{Q9}] Propose how you would solve this using projected gradient descent. Remember that later, you will code this up, so include any detail here you would need to write this code.

\end{itemize}

\section{Multiclass classification}
Now open \texttt{svm\_POS\_release.ipynb} and go through the notebook. Report in your writeup
\begin{itemize}
\item   loss, train and test misclassification  plots for noun-vs-all, for primal projected subgradient method
\item   comparison of loss functions for primal and dual linear SVM
\item   evidence of cross validation to determine best value of $C$ in the primal linear SVM and $C$ and $\sigma$ in Kernel SVM
\item   confusion matrix of final answer on train, validation, and test set.
\end{itemize}

\section{Discussion}
Conclude your report with some observations and thoughts on 
\begin{itemize}
\item the computational complexity of each method
\item the effectiveness of each method
\item any tricks used to deal with imbalanced data, or with overfitting.
\end{itemize}

Would you recommend using word embedding for POS classification, using the method we proposed here? Using a different method? Not at all?
\end{document}